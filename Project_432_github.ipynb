{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d6d26b-d4fa-4b2d-8123-6eb0275d4c30",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb07ca7-bb6b-4561-9adb-6a8512730eb5",
   "metadata": {},
   "source": [
    "## Creating combined data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f319ae-16f8-4b29-84a3-7d1718ff26c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2033b8-6910-4518-8c09-14956dc6c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/User/Downloads/Train_2,4\"\n",
    "dataframes = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path, sep=r'\\s+', header=None)\n",
    "        df['source_file'] = file_name\n",
    "        dataframes.append(df)\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "df=pd.DataFrame(combined_df)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b5566e-6be5-4469-bc88-b45f8afe7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['engine', 'cycle',\n",
    "'setting1', 'setting2', 'setting3',\n",
    "\"Fan_inlet_temperature\",\n",
    "\"LPC_outlet_temperature\",\n",
    "\"HPC_outlet_temperature\",\n",
    "\"LPT_outlet_temperature\",\n",
    "\"Fan_inlet_Pressure\",\n",
    "\"bypass_duct_pressure\",\n",
    "\"HPC_outlet_pressure\",\n",
    "\"Physical_fan_speed\",\n",
    "\"Physical_core_speed\",\n",
    "\"Engine_pressure_ratio\",\n",
    "\"HPC_outlet_Static_pressure\",\n",
    "\"Ratio_of_fuel_flow_to_Ps30\",\n",
    "\"Corrected_fan_speed\",\n",
    "\"Corrected_core_speed\",\n",
    "\"Bypass_Ratio\",\n",
    "\"Burner_fuel_air_ratio\",\n",
    "\"Bleed_Enthalpy\",\n",
    "\"Required_fan_speed\",\n",
    "\"Required_fan_conversion_speed\",\n",
    "\"High_pressure_turbines_Cool_air_flow\",\n",
    "\"Low_pressure_turbines_Cool_air_flow\",\"source_file\" ]\n",
    "df.columns=columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a29312d-b526-46ee-80a3-fc21cb942036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UID for each engine\n",
    "source_file_mapping = {\n",
    "    \"train_FD001.txt\": \"_1\",\n",
    "    \"train_FD002.txt\": \"_2\",\n",
    "    \"train_FD003.txt\": \"_3\",\n",
    "    \"train_FD004.txt\": \"_4\"\n",
    "}\n",
    "df['source_suffix'] = df['source_file'].map(source_file_mapping)\n",
    "df['UID'] = df['engine'].astype(str) + df['source_suffix']\n",
    "df.drop(columns=['source_suffix'], inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e970826-f90c-42cc-b710-2a766a206cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_uid_counts = df.groupby('source_file')['UID'].nunique()\n",
    "print(unique_uid_counts)\n",
    "#double checking with Kaggle values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b909180f-ffbe-4990-a95f-eab1834798d6",
   "metadata": {},
   "source": [
    "# Creating RUL and Failure Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955080e0-0227-4282-8bc1-7a9daffdd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rul'] = df.groupby(['UID'])['cycle'].transform('max') - df['cycle']\n",
    "df['max_cycles'] = df.groupby(['UID'])['cycle'].transform('max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a00fbb-1030-4820-9973-5f6746093c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HPC Degradation'] = 0\n",
    "df['Fan Degradation'] = 0\n",
    "\n",
    "# Apply conditions\n",
    "df.loc[(df['rul'] == 0) & (df['source_file'].isin([\"train_FD001.txt\"])), 'HPC Degradation'] = 1\n",
    "df.loc[(df['rul'] == 0) & (df['source_file'].isin([\"train_FD003.txt\"])), ['HPC Degradation', 'Fan Degradation']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b75c660-802f-40ed-ade7-eb5b51480e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bcec80-fb77-4ad9-b7f0-db8fa57d78e9",
   "metadata": {},
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f6a32-0301-4aea-9732-5d8eb5b6942c",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773788c-6357-4718-8c00-508f8c3398fb",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "No missing values so imputation/dropping of observations required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1520250d-1d9c-4c35-8e07-6164f4d94cb2",
   "metadata": {},
   "source": [
    "## Should we use a combined dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0468a-1bd9-49a4-bca9-87aa8b199eab",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As seen from the TSNE plots datasets 03  & 01 are very distinct from 04 and 02 and therefore we will treat them as such/will create an algorithm that first differentiates on these. Within these, there are alot of overlaps which may result in multicolinearity issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcddd34-8ecb-45b8-825e-9ac12dd03d24",
   "metadata": {},
   "source": [
    "## Checking and Removing outliers using cooks distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb74d1f-f1d0-4f1f-85e2-25782438e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original=df.copy()\n",
    "variables_of_interest = ['setting1', 'setting2', 'setting3',\n",
    "\"Fan_inlet_temperature\",\n",
    "\"LPC_outlet_temperature\",\n",
    "\"HPC_outlet_temperature\",\n",
    "\"LPT_outlet_temperature\",\n",
    "\"Fan_inlet_Pressure\",\n",
    "\"bypass_duct_pressure\",\n",
    "\"HPC_outlet_pressure\",\n",
    "\"Physical_fan_speed\",\n",
    "\"Physical_core_speed\",\n",
    "\"Engine_pressure_ratio\",\n",
    "\"HPC_outlet_Static_pressure\",\n",
    "\"Ratio_of_fuel_flow_to_Ps30\",\n",
    "\"Corrected_fan_speed\",\n",
    "\"Corrected_core_speed\",\n",
    "\"Bypass_Ratio\",\n",
    "\"Burner_fuel_air_ratio\",\n",
    "\"Bleed_Enthalpy\",\n",
    "\"Required_fan_speed\",\n",
    "\"Required_fan_conversion_speed\",\n",
    "\"High_pressure_turbines_Cool_air_flow\",\n",
    "\"Low_pressure_turbines_Cool_air_flow\",\"rul\",'HPC Degradation', 'Fan Degradation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba172624-f19d-40b5-b648-f0be556caeb2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Box plot of physical fan speed, corrected core speed, bypass ratio, required fan speed and RUL seem to suggest presence of outliers. Let us evaluate them further using cook's distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa06c0a-ab1c-42ee-8e44-e945935a6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[variables_of_interest]\n",
    "\n",
    "y = df['rul'] \n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "influence = model.get_influence()\n",
    "cooks_d, p_values = influence.cooks_distance\n",
    "threshold = 1\n",
    "outliers = np.where(cooks_d > threshold)[0]\n",
    "print(f\"Number of outliers detected: {len(outliers)}\")\n",
    "print(f\"Outlier indices: {outliers}\")\n",
    "df_cleaned = df.drop(index=outliers)\n",
    "print(f\"Cleaned dataset shape: {df_cleaned.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058c93df-ccab-4c24-8977-e9152c08ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.stem(np.arange(len(cooks_d)), cooks_d)\n",
    "plt.axhline(y=threshold, color='r', linestyle='--', label='Threshold')\n",
    "plt.title(\"Cook's Distance\")\n",
    "plt.xlabel(\"Observation Index\")\n",
    "plt.ylabel(\"Cook's Distance\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5112b2b-bdda-416e-b9eb-6fd99db1c28c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Based on cook's distance we infer that the data does not have any outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f722ac8-b51c-486b-8dd7-1a49e6221302",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449d53b9-0c37-4d29-9bbe-79c38a465402",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb6b8d3-25ce-4930-8551-8eaef843b858",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* Variables have a large range of values so scaling will be required\n",
    "* Some variables (e.g. setting 2, sensor Engine pressure ratio,Bypass Ratio,Burner fuel-air ratio) have a very small standard deviation/range of values, so they may not be very useful for prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67162a89-97dd-4fa5-b320-e7811452298d",
   "metadata": {},
   "source": [
    "# Corelation between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ec6a8-8f87-4a41-bdcd-c86de8fbeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df[variables_of_interest].corr(), annot=True, cmap='RdYlGn')\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577047a-8a49-447e-999e-01ee1d2eacd0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "* the corellation with RUL for all variables are relatively low while among variables, there is very high corelation\n",
    "    * Therefore we can perform prediction with a small subset of variables (sensor and setting)\n",
    "    * A simple linear regression may not be very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b6e19-ed21-48d9-a4ac-41375749568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_files = df[\"source_file\"].unique()\n",
    "\n",
    "# Generate separate correlation matrices for each source file\n",
    "for source in source_files:\n",
    "    # Filter data for the current source file\n",
    "    df_subset = df[df[\"source_file\"] == source][variables_of_interest]\n",
    "    \n",
    "    # Compute correlation matrix\n",
    "    corr_matrix = df_subset.corr()\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='RdYlGn', fmt=\".2f\", vmin=-1, vmax=1)\n",
    "    plt.title(f\"Correlation Matrix for Source File: {source}\")\n",
    "    plt.grid(False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26b9f2-a04b-4727-8ad2-8e73b1eb8f12",
   "metadata": {},
   "source": [
    "## \n",
    "Correlations are much stronger in datasets 02&04 vs 01&03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca207c95-6b29-4ec2-b855-9539c3650fb4",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67592c-8dce-46b6-a424-184df4e2c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Identify features to scale, excluding specified variables\n",
    "features_to_scale = [var for var in variables_of_interest if var not in ['rul', 'setting1', 'setting2', 'setting3','HPC Degradation', 'Fan Degradation']]\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the selected features\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "# Print the head of the DataFrame to verify the scaling\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106f0812-2045-47ef-afb7-54835ad134a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = variables_of_interest\n",
    "variables_to_drop = [\"rul\",\"setting1\",\"setting2\",\"setting3\"]\n",
    "sensor_columns = [var for var in variables if var not in variables_to_drop]\n",
    "sensor_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4ad9b-f27a-45f8-a12f-19067f5ebac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of sensor columns to plot\n",
    "sensor_columns = [col for col in df.columns if col not in ['UID', 'engine', 'cycle', 'rul', 'max_cycles', 'source_file', \n",
    "                                                           'settings_category', 'setting1', 'setting2', 'setting3']]\n",
    "\n",
    "# Loop through each sensor column\n",
    "for sensor in sensor_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='settings_category', y=sensor, data=df)\n",
    "    plt.title(f'{sensor} vs Settings Category')\n",
    "    plt.xlabel('Settings Category')\n",
    "    plt.ylabel(sensor)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213d05c-e34d-47b3-8294-178c413ac268",
   "metadata": {},
   "source": [
    "## conclusion\n",
    "\n",
    "* sensor value vary randomly for FD002 & FD004\n",
    "  \n",
    "* For FD001\n",
    "    *  required fan speed/ required fan conversion speed/fan inlet temp/fan inlet pressure/ engine pressure ratio/ burner fuel air ratio remains static\n",
    "    *  lpc outlet temp / hpc outlet temp/ lpt outlet temp/ physical fan speed/ HPC outlet static pressure/ corrected fans speed / bypass ratio / bleed enthalpy/ Increases\n",
    "    *  HPC outlet pressure/ ratio of fuel to flow/ HPT cool air flow / LPT cool air flow Decreases\n",
    "\n",
    "* For FD002\n",
    "    *  required fan speed/ required fan conversion speed/fan inlet temp/fan inlet pressure/ engine pressure ratio/ burner fuel air ratio remains static\n",
    "    *  lpc outlet temp / hpc outlet temp/ lpt outlet temp/ HPC outlet static pressure/ corrected fans speed / bleed enthalpy/ Increases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde72632-4db4-4ec2-9ff5-bc6a7ee3828f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "There seems to be more corellation with RUL and setting combinations vs individual settings so we will create a data feature representing this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df142b0a-6093-4188-b7a9-81ccd7afb87c",
   "metadata": {},
   "source": [
    "## Creating additional feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131f1ce4-3812-4722-aae2-7874e688e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predefined conditions with their setting values\n",
    "conditions = [\n",
    "    {'condition': 'Condition_1', 'setting1': 0, 'setting2': 0, 'setting3': 100},\n",
    "    {'condition': 'Condition_2', 'setting1': 10, 'setting2': 0.25, 'setting3': 100},\n",
    "    {'condition': 'Condition_3', 'setting1': 20, 'setting2': 0.7, 'setting3': 100},\n",
    "    {'condition': 'Condition_4', 'setting1': 25, 'setting2': 0.62, 'setting3': 60},\n",
    "    {'condition': 'Condition_5', 'setting1': 35, 'setting2': 0.84, 'setting3': 100},\n",
    "    {'condition': 'Condition_6', 'setting1': 42, 'setting2': 0.84, 'setting3': 100},\n",
    "]\n",
    "\n",
    "# Function to assign the closest condition\n",
    "def assign_closest_condition(row):\n",
    "    min_distance = float('inf')\n",
    "    closest_condition = None\n",
    "    for cond in conditions:\n",
    "        # Calculate total absolute distance across all settings\n",
    "        distance = abs(row['setting1'] - cond['setting1']) + \\\n",
    "                   abs(row['setting2'] - cond['setting2']) + \\\n",
    "                   abs(row['setting3'] - cond['setting3'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_condition = cond['condition']\n",
    "    return closest_condition\n",
    "\n",
    "# Apply the function to the training DataFrame\n",
    "df['settings_category'] = df.apply(assign_closest_condition, axis=1)\n",
    "df['settings_category'] = df['settings_category'].astype('category')\n",
    "\n",
    "# Display unique categories in df\n",
    "print(\"Unique Settings Categories in df:\")\n",
    "print(df['settings_category'].unique())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e624f535-7753-4c09-a482-bb58be3b8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_with_77 = df[df['settings_category'] == 'Condition_6']\n",
    "print(\"Shape of rows with settings_category = 77:\", rows_with_77.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0243e62a-0d42-4131-bbdc-f864b3481c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864115aa-4606-4765-b4d8-206d509f1398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data to include only sensor columns and RUL\n",
    "data = df[sensor_columns + [\"engine\", \"rul\"]]\n",
    "\n",
    "# Loop through each sensor\n",
    "for sensor in sensor_columns:\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    \n",
    "    # Plot each engine's data\n",
    "    for engine_id in data['engine'].unique():\n",
    "        engine_data = data[data['engine'] == engine_id]\n",
    "        \n",
    "        # Compute rolling mean for smoother visualization\n",
    "        rolled_data = engine_data.rolling(window=8).mean()\n",
    "        \n",
    "        # Plot RUL vs sensor value for the current engine\n",
    "        plt.plot(rolled_data['rul'], rolled_data[sensor], alpha=0.6, label=f'Engine {engine_id}')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlim(data['rul'].max(), 0)  # Reverse RUL axis\n",
    "    plt.title(f'{sensor} vs RUL', fontsize=6)\n",
    "    plt.xlabel('Remaining Useful Life (RUL)', fontsize=6)\n",
    "    plt.ylabel(sensor, fontsize=6)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db20f28f-e42a-4768-9d8c-91b3341e1e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# List of sensor columns to plot\n",
    "sensor_columns = [col for col in df.columns if col not in ['UID', 'engine', 'cycle', 'rul', 'max_cycles', 'source_file', \n",
    "                                                           'settings_category', 'setting1', 'setting2', 'setting3']]\n",
    "\n",
    "# Loop through each sensor column\n",
    "for sensor in sensor_columns:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x='settings_category', y=sensor, data=df)\n",
    "    plt.title(f'{sensor} vs Settings Category')\n",
    "    plt.xlabel('Settings Category')\n",
    "    plt.ylabel(sensor)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f93cb-ea46-4dd5-84ab-a16ef8093732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use the 'settings_category' column for grouping\n",
    "settings_categories = df['settings_category'].unique()  # ['Condition_1', ..., 'Condition_6']\n",
    "\n",
    "# Loop through each sensor\n",
    "for sensor in sensor_columns:\n",
    "    # Set up subplots\n",
    "    num_categories = len(settings_categories)\n",
    "    num_cols = 3  # Number of columns in the subplot grid\n",
    "    num_rows = -(-num_categories // num_cols)  # Calculate number of rows (ceil division)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "    axes = axes.flatten()  # Flatten axes for easier iteration\n",
    "\n",
    "    # Loop through unique settings categories\n",
    "    for idx, category in enumerate(settings_categories):\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df['settings_category'] == category]\n",
    "\n",
    "        # Get the axis for the current category\n",
    "        ax = axes[idx]\n",
    "\n",
    "        # Plot RUL vs sensor for the current category\n",
    "        sns.lineplot(\n",
    "            data=category_data,\n",
    "            x=\"rul\",\n",
    "            y=sensor,\n",
    "            ax=ax,\n",
    "            alpha=0.6\n",
    "        )\n",
    "\n",
    "        # Customize the plot\n",
    "        ax.set_title(f\"{sensor} vs RUL for {category}\")\n",
    "        ax.set_xlabel(\"Remaining Useful Life (RUL)\")\n",
    "        ax.set_ylabel(sensor)\n",
    "        ax.grid(True)\n",
    "\n",
    "    # Remove unused subplots\n",
    "    for idx in range(num_categories, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb2cc0f-2206-41a1-8b4e-15ecee99af09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterate through each unique settings_category\n",
    "unique_categories = df['settings_category'].unique()\n",
    "\n",
    "for category in unique_categories:\n",
    "    # Filter data for the current category\n",
    "    category_data = df[df['settings_category'] == category]\n",
    "    \n",
    "    # Get the unique engine IDs within this category\n",
    "    engine_ids = category_data['UID'].unique()\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Iterate through each engine in the category\n",
    "    for engine_id in engine_ids:\n",
    "        # Filter data for the current engine\n",
    "        engine_data = category_data[category_data['UID'] == engine_id]\n",
    "        \n",
    "        # Plot RUL vs Cycle for this engine\n",
    "        plt.plot(engine_data['cycle'], engine_data['rul'], label=f'Engine {engine_id}', alpha=0.6)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'RUL Trends for Engines in {category}', fontsize=16)\n",
    "    plt.xlabel('Cycle', fontsize=14)\n",
    "    plt.ylabel('Remaining Useful Life (RUL)', fontsize=14)\n",
    "    plt.legend(title='Engines', loc='upper right', fontsize=10, bbox_to_anchor=(1.15, 1))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f59dc4-68e9-4d33-ab6e-17d79659feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Iterate through each unique settings_category\n",
    "unique_categories = df['settings_category'].unique()\n",
    "\n",
    "for category in unique_categories:\n",
    "    # Filter data for the current category\n",
    "    category_data = df[df['settings_category'] == category]\n",
    "    \n",
    "    # Get the unique engine IDs within this category\n",
    "    engine_ids = category_data['UID'].unique()\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(14, 8))  # Increase figure size to allow more space for decorations\n",
    "    \n",
    "    # Iterate through each engine in the category\n",
    "    for engine_id in engine_ids:\n",
    "        # Filter data for the current engine\n",
    "        engine_data = category_data[category_data['UID'] == engine_id]\n",
    "        \n",
    "        # Plot RUL vs Cycle for this engine\n",
    "        plt.plot(engine_data['cycle'], engine_data['rul'], label=f'Engine {engine_id}', alpha=0.6)\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f'RUL Trends for Engines in {category}', fontsize=6, pad=20)  # Add padding to the title\n",
    "    plt.xlabel('Cycle', fontsize=4)\n",
    "    plt.ylabel('Remaining Useful Life (RUL)', fontsize=4)\n",
    "    plt.legend(title='Engines', loc='upper right', fontsize=10, bbox_to_anchor=(1.15, 1))\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Adjust layout\n",
    "    plt.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)  # Adjust margins manually\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743282f3-f1bc-47db-85b6-80829f8142a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of sensors (replace with actual sensor columns from your dataset)\n",
    "sensor_columns = [\n",
    "    \"Fan_inlet_temperature\",\n",
    "    \"LPC_outlet_temperature\",\n",
    "    \"HPC_outlet_temperature\",\n",
    "    \"LPT_outlet_temperature\",\n",
    "    \"Fan_inlet_Pressure\",\n",
    "    \"bypass_duct_pressure\",\n",
    "    \"HPC_outlet_pressure\",\n",
    "    \"Physical_fan_speed\",\n",
    "    \"Physical_core_speed\",\n",
    "    \"Engine_pressure_ratio\",\n",
    "    \"HPC_outlet_Static_pressure\",\n",
    "    \"Ratio_of_fuel_flow_to_Ps30\",\n",
    "    \"Corrected_fan_speed\",\n",
    "    \"Corrected_core_speed\",\n",
    "    \"Bypass_Ratio\",\n",
    "    \"Burner_fuel_air_ratio\",\n",
    "    \"Bleed_Enthalpy\",\n",
    "    \"Required_fan_speed\",\n",
    "    \"Required_fan_conversion_speed\",\n",
    "    \"High_pressure_turbines_Cool_air_flow\",\n",
    "    \"Low_pressure_turbines_Cool_air_flow\",\n",
    "]\n",
    "\n",
    "# Iterate through each sensor\n",
    "for sensor in sensor_columns:\n",
    "    # Iterate through each unique settings category\n",
    "    unique_categories = df[\"settings_category\"].unique()\n",
    "    \n",
    "    for category in unique_categories:\n",
    "        # Filter data for the current category\n",
    "        category_data = df[df[\"settings_category\"] == category]\n",
    "        \n",
    "        # Get unique engine IDs within the current category\n",
    "        engine_ids = category_data[\"UID\"].unique()\n",
    "        \n",
    "        # Set up the plot\n",
    "        plt.figure(figsize=(4,2))  # Adjust figure size\n",
    "        \n",
    "        # Iterate through each engine in the category\n",
    "        for engine_id in engine_ids:\n",
    "            # Filter data for the current engine\n",
    "            engine_data = category_data[category_data[\"UID\"] == engine_id]\n",
    "            \n",
    "            # Plot RUL vs sensor values for this engine\n",
    "            plt.plot(\n",
    "                engine_data[sensor],\n",
    "                engine_data[\"rul\"],\n",
    "                alpha=0.6,\n",
    "            )\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title(f\"RUL Trends for {sensor} in {category}\", fontsize=6, pad=20)\n",
    "        plt.xlabel(sensor, fontsize=6)\n",
    "        plt.ylabel(\"Remaining Useful Life (RUL)\", fontsize=6)\n",
    "        plt.legend(title=\"Engines\", loc=\"upper right\", fontsize=10, bbox_to_anchor=(1.15, 1))\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Adjust layout\n",
    "        plt.subplots_adjust(bottom=0.1, top=0.9, left=0.1, right=0.9)  # Adjust margins manually\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1494d75d-ee0e-46a1-a32c-3ee538141a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List of sensor columns (replace with actual column names from your dataset)\n",
    "sensor_columns = [\n",
    "    \"Fan_inlet_temperature\",\n",
    "    \"LPC_outlet_temperature\",\n",
    "    \"HPC_outlet_temperature\",\n",
    "    \"LPT_outlet_temperature\",\n",
    "    \"Fan_inlet_Pressure\",\n",
    "    \"bypass_duct_pressure\",\n",
    "    \"HPC_outlet_pressure\",\n",
    "    \"Physical_fan_speed\",\n",
    "    \"Physical_core_speed\",\n",
    "    \"Engine_pressure_ratio\",\n",
    "    \"HPC_outlet_Static_pressure\",\n",
    "    \"Ratio_of_fuel_flow_to_Ps30\",\n",
    "    \"Corrected_fan_speed\",\n",
    "    \"Corrected_core_speed\",\n",
    "    \"Bypass_Ratio\",\n",
    "    \"Burner_fuel_air_ratio\",\n",
    "    \"Bleed_Enthalpy\",\n",
    "    \"Required_fan_speed\",\n",
    "    \"Required_fan_conversion_speed\",\n",
    "    \"High_pressure_turbines_Cool_air_flow\",\n",
    "    \"Low_pressure_turbines_Cool_air_flow\",\n",
    "]\n",
    "\n",
    "# Iterate through each sensor\n",
    "for sensor in sensor_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Scatter plot for RUL vs. the sensor\n",
    "    plt.scatter(df[sensor], df[\"rul\"], alpha=0.6, color=\"blue\", edgecolor=\"k\")\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.title(f\"RUL vs {sensor}\", fontsize=16, pad=20)\n",
    "    plt.xlabel(sensor, fontsize=14)\n",
    "    plt.ylabel(\"Remaining Useful Life (RUL)\", fontsize=14)\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85645a3-3d9d-4aa9-9c1f-53143aad3a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Filter data to include only sensor columns, engine, and RUL\n",
    "data = df[sensor_columns + [\"engine\", \"rul\"]]\n",
    "\n",
    "# Loop through each sensor\n",
    "for sensor in sensor_columns:\n",
    "    engines = data['engine'].unique()\n",
    "    num_engines = len(engines)\n",
    "    num_cols = 4  # 4 plots in a row\n",
    "    num_rows = -(-num_engines // num_cols)  # Calculate rows needed (ceil division)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, num_rows * 5))\n",
    "    axes = axes.flatten()  # Flatten axes for easy indexing\n",
    "    \n",
    "    for idx, engine_id in enumerate(engines):\n",
    "        # Filter data for the current engine\n",
    "        engine_data = data[data['engine'] == engine_id]\n",
    "        \n",
    "        # Compute rolling mean of the sensor values\n",
    "        rolled_data = engine_data.rolling(window=8).mean()\n",
    "        \n",
    "        # Plot rolling mean against RUL\n",
    "        ax = axes[idx]\n",
    "        ax.plot(rolled_data['rul'], rolled_data[sensor], alpha=0.6, label=f'Engine {engine_id}')\n",
    "\n",
    "        # Customize the subplot\n",
    "        ax.set_xlim(data['rul'].max(), 0)\n",
    "        ax.set_xticks(np.arange(0, data['rul'].max() + 1, 25))\n",
    "        ax.set_title(f'{sensor} (Engine: {engine_id})', fontsize=12)\n",
    "        ax.set_xlabel('Remaining Useful Life')\n",
    "        ax.set_ylabel(sensor)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Remove unused subplots\n",
    "    for idx in range(num_engines, len(axes)):\n",
    "        fig.delaxes(axes[idx])\n",
    "\n",
    "    # Add a title to the overall figure\n",
    "    fig.suptitle(f'{sensor} (Rolling Mean of Previous 8 RULs) vs RUL for All Engines', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424af514-3fd0-43cb-a76b-1995d79d6d28",
   "metadata": {},
   "source": [
    "## Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63896066-8163-42fd-b077-6ab252969878",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"C:/Users/User/Downloads/Test_2,4\"\n",
    "dataframes = []\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        test_df = pd.read_csv(file_path, sep=r'\\s+', header=None)\n",
    "        test_df['source_file'] = file_name\n",
    "        dataframes.append(test_df)\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "test_df=pd.DataFrame(combined_df)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fca97c-2311-4b35-8ba0-dc437f1e0d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['UID', 'rul', 'HPC Degradation', 'Fan Degradation','max_cycles','floored_setting1','floored_setting2','floored_setting3','settings_category']\n",
    "new_columns = [col for col in df_original.columns if col not in columns_to_exclude]\n",
    "test_df.columns = new_columns\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76ec80-25f0-4226-ae1f-7ff5cc405e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create UID for each engine\n",
    "source_file_mapping = {\n",
    "    \"test_FD001.txt\": \"_1\",\n",
    "    \"test_FD002.txt\": \"_2\",\n",
    "    \"test_FD003.txt\": \"_3\",\n",
    "    \"test_FD004.txt\": \"_4\"\n",
    "}\n",
    "test_df['source_suffix'] = test_df['source_file'].map(source_file_mapping)\n",
    "test_df['UID'] = test_df['engine'].astype(str) + test_df['source_suffix']\n",
    "test_df.drop(columns=['source_suffix'], inplace=True)\n",
    "print(test_df[['engine', 'source_file', 'UID']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb33b74e-2755-4332-825e-6b857899c8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of predefined conditions with their setting values\n",
    "conditions = [\n",
    "    {'condition': 'Condition_1', 'setting1': 0, 'setting2': 0, 'setting3': 100},\n",
    "    {'condition': 'Condition_2', 'setting1': 10, 'setting2': 0.25, 'setting3': 100},\n",
    "    {'condition': 'Condition_3', 'setting1': 20, 'setting2': 0.7, 'setting3': 100},\n",
    "    {'condition': 'Condition_4', 'setting1': 25, 'setting2': 0.62, 'setting3': 60},\n",
    "    {'condition': 'Condition_5', 'setting1': 35, 'setting2': 0.84, 'setting3': 100},\n",
    "    {'condition': 'Condition_6', 'setting1': 42, 'setting2': 0.84, 'setting3': 100},\n",
    "]\n",
    "\n",
    "# Function to assign the closest condition\n",
    "def assign_closest_condition(row):\n",
    "    min_distance = float('inf')\n",
    "    closest_condition = None\n",
    "    for cond in conditions:\n",
    "        # Calculate total absolute distance across all settings\n",
    "        distance = abs(row['setting1'] - cond['setting1']) + \\\n",
    "                   abs(row['setting2'] - cond['setting2']) + \\\n",
    "                   abs(row['setting3'] - cond['setting3'])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_condition = cond['condition']\n",
    "    return closest_condition\n",
    "\n",
    "# Apply the function to the training DataFrame\n",
    "test_df['settings_category'] = test_df.apply(assign_closest_condition, axis=1)\n",
    "test_df['settings_category'] =test_df['settings_category'].astype('category')\n",
    "\n",
    "# Display unique categories in test_df\n",
    "print(\"Unique Settings Categories in test_df:\")\n",
    "print(test_df['settings_category'].unique())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5617cd9-e5f1-47bb-9e56-044d1ed72e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "test_df[features_to_scale] = scaler.fit_transform(test_df[features_to_scale])\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fbfe5-8b6a-4828-87a8-a844da06c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_rul =\"C:/Users/User/Downloads/RUL_2,4\"\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Iterate over all files in the folder\n",
    "for file_name_rul in os.listdir(folder_path_rul):\n",
    "    if file_name_rul.endswith('.txt'):  # Check if the file is a .txt file\n",
    "        file_path_rul = os.path.join(folder_path_rul, file_name_rul)\n",
    "        \n",
    "        # Read the file into a DataFrame\n",
    "        RUL_df = pd.read_csv(file_path_rul, sep=r'\\s+', header=None)\n",
    "        print(RUL_df.shape)\n",
    "        # Add a column for the source file name\n",
    "        RUL_df['source_file'] = file_name_rul\n",
    "        # Append the DataFrame to the list\n",
    "        dataframes.append(RUL_df)\n",
    "\n",
    "# Combine all DataFrames into one\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Convert to a DataFrame (optional, since `combined_df` is already a DataFrame)\n",
    "RUL_df = pd.DataFrame(combined_df)\n",
    "\n",
    "# Print the head of the combined DataFrame\n",
    "RUL_df.columns = ['rul','source_file'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7129c-ce37-4e47-9344-2791afca9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping for source files\n",
    "source_file_mapping = {\n",
    "    \"RUL_FD001.txt\": \"_1\",\n",
    "    \"RUL_FD002.txt\": \"_2\",\n",
    "    \"RUL_FD003.txt\": \"_3\",\n",
    "    \"RUL_FD004.txt\": \"_4\"\n",
    "}\n",
    "\n",
    "# Map the source_file column to its corresponding suffix\n",
    "RUL_df['source_suffix'] = RUL_df['source_file'].map(source_file_mapping)\n",
    "\n",
    "# Create a sequential number (1, 2, 3, ...) for each source_file\n",
    "RUL_df['engine_seq'] = RUL_df.groupby('source_file').cumcount() + 1\n",
    "\n",
    "# Combine the sequential number and source suffix to create the UID\n",
    "RUL_df['UID'] = RUL_df['engine_seq'].astype(str) + RUL_df['source_suffix']\n",
    "\n",
    "# Drop temporary columns if not needed\n",
    "RUL_df.drop(columns=['source_suffix'], inplace=True)\n",
    "\n",
    "# Preview the resulting DataFrame\n",
    "print(RUL_df[['engine_seq', 'source_file', 'UID']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba0f0f0-3941-41bb-990c-85c1ed50ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb44a8-e09f-4d09-8790-4b9cda4375d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(RUL_df.shape)\n",
    "unique_uid_counts = test_df.groupby('source_file')['UID'].nunique()\n",
    "print(sum(unique_uid_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c8236-f379-4470-a300-7e909c2740f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the dataframes on 'UID', keeping all columns and filtering only matching UIDs\n",
    "merged_df = test_df.merge(RUL_df, on='UID', how='inner')\n",
    "\n",
    "# Perform a left join to retain all rows from clustered_test_df\n",
    "left_join_df = test_df.merge(RUL_df, on='UID', how='left', indicator=True)\n",
    "\n",
    "# Count rows in clustered_test_df that didn't find a match in RUL_test\n",
    "non_matching_rows = left_join_df[left_join_df['_merge'] == 'left_only'].shape[0]\n",
    "\n",
    "# Drop the indicator column (optional)\n",
    "left_join_df.drop(columns=['_merge'], inplace=True)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Number of rows in clustered_test_df that didn't find a match: {non_matching_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9962803-8583-47ac-b055-c411709110ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5c6a09-6a5f-4f46-a0af-aaeebaa6fb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column for max_cycles to the training and testing datasets\n",
    "\n",
    "test_df['max_cycles'] = test_df.groupby(['UID'])['cycle'].transform('max')\n",
    "\n",
    "# Calculate RUL as max_cycles - current cycle\n",
    "\n",
    "test_df['rul'] = test_df['max_cycles'] - test_df['cycle']\n",
    "\n",
    "print(test_df[['UID', 'cycle', 'max_cycles', 'rul']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa9a275-c6e4-4c54-9b04-816b7d858657",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Mohini edited\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Identify sensor columns (exclude specific columns)\n",
    "excluded_columns = ['engine', 'cycle', 'UID', 'rul', 'max_cycles', 'source_file', 'settings_category',\n",
    "                    'setting1', 'setting2', 'setting3',\n",
    "                    'HPC Degradation', 'Fan Degradation',\n",
    "                    'floored_setting1', 'floored_setting2', 'floored_setting3']\n",
    "sensor_columns = [col for col in df.columns if col not in excluded_columns]\n",
    "\n",
    "all_features_with_correlations = []\n",
    "\n",
    "# Step 2: Process each sensor column, grouped by `settings_category`\n",
    "for feature in sensor_columns:\n",
    "    try:\n",
    "        grouped_correlations = []\n",
    "\n",
    "        # Iterate through each unique `settings_category`\n",
    "        for category in df['settings_category'].unique():\n",
    "            df_filtered = df[df['settings_category'] == category]\n",
    "\n",
    "            # Check correlation of the original feature\n",
    "            original_correlation = np.corrcoef(df_filtered[feature], df_filtered[\"max_cycles\"])[0, 1]\n",
    "            best_correlation = original_correlation\n",
    "            best_feature_name = feature\n",
    "\n",
    "            # Generate polynomial transformations and check correlations\n",
    "            poly = PolynomialFeatures(degree=10, include_bias=False)\n",
    "            transformed = poly.fit_transform(df_filtered[[feature]])\n",
    "            feature_names = poly.get_feature_names_out([feature])\n",
    "\n",
    "            for i, transformed_column in enumerate(transformed.T):\n",
    "                correlation = np.corrcoef(transformed_column, df_filtered[\"max_cycles\"])[0, 1]\n",
    "                if abs(correlation) > abs(best_correlation):\n",
    "                    best_correlation = correlation\n",
    "                    best_feature_name = feature_names[i]\n",
    "\n",
    "            grouped_correlations.append(best_correlation)\n",
    "\n",
    "        # Calculate the average correlation across all `settings_category`\n",
    "        avg_correlation = np.mean(grouped_correlations)\n",
    "\n",
    "        # Apply correlation threshold\n",
    "        if abs(avg_correlation) > 0.05:\n",
    "            all_features_with_correlations.append((feature, best_feature_name, avg_correlation))\n",
    "            print(f\"Sensor: {feature}, Best Feature: {best_feature_name}, Average Correlation: {avg_correlation:.4f}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {feature} due to error: {e}\")\n",
    "\n",
    "# Step 3: Sort all features by absolute average correlation and select top 10\n",
    "sorted_features = sorted(all_features_with_correlations, key=lambda x: abs(x[2]), reverse=True)\n",
    "top_10_features = sorted_features[:10]\n",
    "\n",
    "# Step 4: Display results\n",
    "print(\"\\nTop 10 Features Based on Average Correlation with max_cycles (Grouped by settings_category):\")\n",
    "for feature, name, correlation in top_10_features:\n",
    "    print(f\"Sensor: {feature}, Selected Feature: {name}, Average Correlation: {correlation:.4f}\")\n",
    "\n",
    "# Step 5: Store final selection\n",
    "best_polynomial_features = {feature: name for feature, name, correlation in top_10_features}\n",
    "\n",
    "print(\"\\nFinal Best Polynomial Features Selected:\")\n",
    "for feature, name in best_polynomial_features.items():\n",
    "    print(f\"Sensor: {feature}, Selected Feature: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce5fb4-fc22-485a-a7eb-9cf54936b3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Step 1: Add the best features to the training and testing datasets\n",
    "for original_feature, selected_feature in best_polynomial_features.items():\n",
    "    degree = int(selected_feature.split(\"^\")[-1]) if \"^\" in selected_feature else 1\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Add to training dataset\n",
    "    transformed_train = poly.fit_transform(df[[original_feature]])\n",
    "    train_feature_names = poly.get_feature_names_out([original_feature])\n",
    "    df[selected_feature] = transformed_train[:, train_feature_names.tolist().index(selected_feature)]\n",
    "    \n",
    "    # Add to testing dataset\n",
    "    transformed_test = poly.fit_transform(test_df[[original_feature]])\n",
    "    test_feature_names = poly.get_feature_names_out([original_feature])\n",
    "    test_df[selected_feature] = transformed_test[:, test_feature_names.tolist().index(selected_feature)]\n",
    "\n",
    "# Confirm feature addition\n",
    "print(f\"Number of Columns in Training Dataset After Adding Features: {df.shape[1]}\")\n",
    "print(f\"Number of Columns in Testing Dataset After Adding Features: {test_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a68a64e-6ea4-4d3d-ae25-d7c98936b34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# One-hot encode `settings_category`\n",
    "encoder = OneHotEncoder(drop=\"first\", sparse_output=False)\n",
    "\n",
    "# Apply one-hot encoding for training and testing datasets\n",
    "settings_encoded_train = pd.DataFrame(\n",
    "    encoder.fit_transform(df[[\"settings_category\"]]),\n",
    "    columns=encoder.get_feature_names_out([\"settings_category\"]),\n",
    "    index=df.index\n",
    ")\n",
    "settings_encoded_test = pd.DataFrame(\n",
    "    encoder.transform(test_df[[\"settings_category\"]]),\n",
    "    columns=encoder.get_feature_names_out([\"settings_category\"]),\n",
    "    index=test_df.index\n",
    ")\n",
    "\n",
    "# Add the encoded columns to the datasets\n",
    "df = pd.concat([df, settings_encoded_train], axis=1)\n",
    "test_df = pd.concat([test_df, settings_encoded_test], axis=1)\n",
    "\n",
    "# Remove the original `settings_category` column\n",
    "df.drop(columns=[\"settings_category\"], inplace=True)\n",
    "test_df.drop(columns=[\"settings_category\"], inplace=True)\n",
    "\n",
    "# Define final selected features\n",
    "selected_columns = [\"UID\", \"engine\", \"cycle\", \"rul\", \"max_cycles\"] + list(settings_encoded_train.columns)\n",
    "selected_features = selected_columns + list(best_polynomial_features.values())\n",
    "\n",
    "# Combine selected features for averaging\n",
    "# Use max for cycle, mean for numeric columns\n",
    "df_grouped_train = (\n",
    "    df[selected_features]\n",
    "    .groupby(\"UID\", as_index=False)\n",
    "    .agg(\n",
    "        {col: (\"max\" if col == \"cycle\" else \"mean\") for col in selected_features if col != \"UID\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "df_grouped_test = (\n",
    "    test_df[selected_features]\n",
    "    .groupby(\"UID\", as_index=False)\n",
    "    .agg(\n",
    "        {col: (\"max\" if col == \"cycle\" else \"mean\") for col in selected_features if col != \"UID\"}\n",
    "    )\n",
    ")\n",
    "\n",
    "# Display resulting shapes\n",
    "print(\"Final Grouped Train Features Shape:\", df_grouped_train.shape)\n",
    "print(\"Final Grouped Test Features Shape:\", df_grouped_test.shape)\n",
    "\n",
    "# Optionally display a sample for verification\n",
    "print(df_grouped_train.head())\n",
    "print(df_grouped_test.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d46499-f1d8-42e2-a34a-aa0673da92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24ccd8d-6fdb-496f-8973-1eb1b068c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictors and response variable\n",
    "X_train = df_grouped_train.drop(columns=[\"UID\", \"max_cycles\",\"engine\"])  # Remove non-predictor columns\n",
    "y_train = df_grouped_train[\"max_cycles\"]  # Set RUL as the target variable\n",
    "\n",
    "X_test = df_grouped_test.drop(columns=[\"UID\", \"max_cycles\",\"engine\"])\n",
    "\n",
    "\n",
    "# Display shapes for verification\n",
    "print(\"Training Features Shape:\", X_train.shape)\n",
    "print(\"Training Target Shape (max_cycles):\", y_train.shape)\n",
    "print(\"Testing Features Shape:\", X_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0471a671-0912-4446-a66b-f38e327129c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_check = X_train.columns\n",
    "print(column_names_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6a5d35-2ec0-4505-a64d-b117174e6bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85d193f-d970-4130-88a2-d6e30bfafd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Forest Regressor\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=200,  # Number of trees\n",
    "    max_depth=15,      # Maximum depth of trees\n",
    "    random_state=42    # Ensures reproducibility\n",
    ")\n",
    "\n",
    "# Train the model on training data\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "# Display confirmation\n",
    "print(\"Random Forest Model Trained Successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ee8582-27b2-4641-a169-5337f2cf7449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on training and testing datasets\n",
    "y_train_pred = rfr.predict(X_train)\n",
    "y_test_pred = rfr.predict(X_test)# add back to X_test then unaggregate it and compare against test_df rul by y_test_pred-cycles Then evavulate metric\n",
    "\n",
    "# Calculate metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "train_rmse = mean_squared_error(y_train, y_train_pred, squared=False)\n",
    "\n",
    "#test_r2 = r2_score(y_test, y_test_pred)\n",
    "#test_rmse = mean_squared_error(y_test, y_test_pred, squared=False)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Train R^2: {train_r2:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "#print(f\"Test R^2: {test_r2:.4f}\")\n",
    "#print(f\"Test RMSE: {test_rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4841beac-70c1-4469-aece-8821a71a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions back to X_test\n",
    "X_test[\"predicted_max_cycles\"] = y_test_pred\n",
    "X_test[\"UID\"] = df_grouped_test[\"UID\"]\n",
    "\n",
    "# Unaggregate X_test to match original test_df\n",
    "# Merge predictions with the original test_df\n",
    "test_df_with_predictions = test_df.merge(\n",
    "    X_test[[\"UID\", \"predicted_max_cycles\"]], \n",
    "    on=\"UID\", \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Calculate RUL predictions by subtracting `cycle` from `predicted_max_cycles`\n",
    "test_df_with_predictions[\"predicted_rul\"] = (\n",
    "    test_df_with_predictions[\"predicted_max_cycles\"] - test_df_with_predictions[\"cycle\"]\n",
    ")\n",
    "\n",
    "# Ensure RUL predictions are non-negative\n",
    "test_df_with_predictions[\"predicted_rul\"] = test_df_with_predictions[\"predicted_rul\"].clip(lower=0)\n",
    "\n",
    "# Calculate metrics on unaggregated data\n",
    "test_r2 = r2_score(test_df_with_predictions[\"rul\"], test_df_with_predictions[\"predicted_rul\"])\n",
    "test_rmse = mean_squared_error(test_df_with_predictions[\"rul\"], test_df_with_predictions[\"predicted_rul\"], squared=False)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(f\"Train R^2: {train_r2:.4f}\")\n",
    "print(f\"Train RMSE: {train_rmse:.4f}\")\n",
    "print(f\"Test R^2: {test_r2:.4f}\")\n",
    "print(f\"Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Optionally display a sample for verification\n",
    "print(test_df_with_predictions[[\"UID\", \"cycle\", \"rul\", \"predicted_rul\"]].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e41bdcd-a2d8-41b2-9f49-5ec323bb8de2",
   "metadata": {},
   "source": [
    "Conjoint Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8609e15-2149-4865-95d1-62b9ac6d9211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for each setting\n",
    "setting1_thresholds = [0, 10, 20, 25, 35, 42]\n",
    "setting2_thresholds = [0, 0.25, 0.7, 0.84]\n",
    "setting3_thresholds = [60, 100]\n",
    "\n",
    "# Function to assign categories based on thresholds\n",
    "def assign_setting_category(value, thresholds):\n",
    "    for i in range(len(thresholds) - 1):\n",
    "        if thresholds[i] <= value < thresholds[i + 1]:\n",
    "            return f\"Category_{i + 1}\"\n",
    "    return f\"Category_{len(thresholds)}\"  # Assign to the last category\n",
    "\n",
    "# Apply the function to assign categories for each setting\n",
    "df[\"setting1_category\"] = df[\"setting1\"].apply(assign_setting_category, thresholds=setting1_thresholds)\n",
    "df[\"setting2_category\"] = df[\"setting2\"].apply(assign_setting_category, thresholds=setting2_thresholds)\n",
    "df[\"setting3_category\"] = df[\"setting3\"].apply(assign_setting_category, thresholds=setting3_thresholds)\n",
    "\n",
    "# Verify the results\n",
    "print(df[[\"setting1_category\", \"setting2_category\", \"setting3_category\"]].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24cba6c-cee1-40a3-8ad3-c9c74f8daa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Initialize separate encoders for each setting category\n",
    "encoder1 = OneHotEncoder(drop=None, sparse_output=False)\n",
    "encoder2 = OneHotEncoder(drop=None, sparse_output=False)\n",
    "encoder3 = OneHotEncoder(drop=None, sparse_output=False)\n",
    "\n",
    "# One-hot encode categories for each setting\n",
    "encoded_setting1 = encoder1.fit_transform(df[[\"setting1_category\"]])\n",
    "encoded_setting2 = encoder2.fit_transform(df[[\"setting2_category\"]])\n",
    "encoded_setting3 = encoder3.fit_transform(df[[\"setting3_category\"]])\n",
    "\n",
    "# Convert encoded arrays to DataFrames with appropriate column names\n",
    "encoded_setting1_df = pd.DataFrame(\n",
    "    encoded_setting1,\n",
    "    columns=encoder1.get_feature_names_out([\"setting1_category\"]),\n",
    "    index=df.index\n",
    ")\n",
    "encoded_setting2_df = pd.DataFrame(\n",
    "    encoded_setting2,\n",
    "    columns=encoder2.get_feature_names_out([\"setting2_category\"]),\n",
    "    index=df.index\n",
    ")\n",
    "encoded_setting3_df = pd.DataFrame(\n",
    "    encoded_setting3,\n",
    "    columns=encoder3.get_feature_names_out([\"setting3_category\"]),\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Combine the original DataFrame with the encoded columns\n",
    "df = pd.concat([df, encoded_setting1_df, encoded_setting2_df, encoded_setting3_df], axis=1)\n",
    "\n",
    "# Drop the original category columns\n",
    "df.drop(columns=[\"setting1_category\", \"setting2_category\", \"setting3_category\"], inplace=True)\n",
    "\n",
    "# Verify the updated DataFrame\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e5e77-9333-4659-8892-91f1a13b191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the one-hot encoded setting category columns as predictors\n",
    "setting_columns = [col for col in df.columns if \"setting1_category\" in col or \n",
    "                   \"setting2_category\" in col or \n",
    "                   \"setting3_category\" in col]\n",
    "\n",
    "X = df[setting_columns]  # Predictors: One-hot encoded setting categories\n",
    "y = df[\"max_cycles\"]  # Response: `max_cycles` as the target variable\n",
    "\n",
    "# Display shapes\n",
    "print(f\"Predictors Shape: {X.shape}\")\n",
    "print(f\"Response Shape: {y.shape}\")\n",
    "\n",
    "# Optional: Display the first few rows of the predictors to verify\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c19a31-38e1-4dd8-9958-b0d6278a7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Initialize and fit the model\n",
    "lm = LinearRegression()\n",
    "lm.fit(X, y)\n",
    "\n",
    "# Display coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": lm.coef_\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"Conjoint Analysis Coefficients:\")\n",
    "print(coefficients)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lm.predict(X)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a20f6e-f8bd-4cb0-b922-5be7f9e06280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot coefficients\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=\"Coefficient\", y=\"Feature\", data=coefficients)\n",
    "plt.title(\"Conjoint Analysis Feature Importance\")\n",
    "plt.xlabel(\"Coefficient\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d55c54-627d-4900-a197-5f2228f67694",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
